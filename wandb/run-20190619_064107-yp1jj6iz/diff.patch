diff --git a/char-gen.py b/char-gen.py
index 0e44c45..c06df42 100644
--- a/char-gen.py
+++ b/char-gen.py
@@ -14,13 +14,14 @@ import argparse
 
 parser = argparse.ArgumentParser()
 parser.add_argument("text", type=str)
+parser.add_argument("batch_size",typr=int)
 
 args = parser.parse_args()
 
 run = wandb.init()
 config = run.config
 config.hidden_nodes = 128
-config.batch_size = 256
+config.batch_size = aegs.batch_size
 config.file = args.text
 config.maxlen = 200
 config.step = 3
diff --git a/imdb-lstm.py b/imdb-lstm.py
index 74206fd..02ccccb 100644
--- a/imdb-lstm.py
+++ b/imdb-lstm.py
@@ -1,7 +1,7 @@
 from keras.preprocessing import sequence
 from keras.models import Sequential
 from keras.layers import Dense, Dropout, Activation
-from keras.layers import Embedding, LSTM, Bidirectional
+from keras.layers import Embedding, CuDNNLSTM, Bidirectional
 from keras.layers import Conv1D, Flatten
 from keras.datasets import imdb
 import wandb
@@ -37,7 +37,7 @@ model = Sequential()
 model.add(Embedding(config.vocab_size,
                     config.embedding_dims,
                     input_length=config.maxlen))
-model.add(LSTM(config.hidden_dims, activation="sigmoid"))
+model.add(CuDNNLSTM(config.hidden_dims))
 model.add(Dense(1, activation='sigmoid'))
 model.compile(loss='binary_crossentropy',
               optimizer='rmsprop',
